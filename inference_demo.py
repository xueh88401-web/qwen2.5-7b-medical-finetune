#!/usr/bin/env python3
"""
Qwen2.5-7B åŒ»å­¦æ¨¡å‹æ¨ç†æ¼”ç¤ºè„šæœ¬ã€‚
ç”¨é€”ï¼š1. å¿«é€Ÿæµ‹è¯•æ¨¡å‹æ•ˆæœï¼›2. ä½œä¸ºé¡¹ç›®ä½¿ç”¨ç¤ºä¾‹ã€‚
ä½¿ç”¨æ–¹æ³•: python inference_demo.py --query "ä½ çš„é—®é¢˜"
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import argparse
import sys
import os

def load_model_and_tokenizer(adapter_path="./adapter", use_hf_adapter=False):
    """
    åŠ è½½åŸºç¡€æ¨¡å‹å’ŒLoRAé€‚é…å™¨ã€‚
    å‚æ•°:
        adapter_path: æœ¬åœ°LoRAé€‚é…å™¨ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º'./adapter'
        use_hf_adapter: æ˜¯å¦ä½¿ç”¨HuggingFaceä¸Šçš„é€‚é…å™¨
    è¿”å›:
        model, tokenizer
    """
    print("=== æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œé¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ... ===")
    
    # åŸºç¡€æ¨¡å‹ID
    base_model_id = "Qwen/Qwen2.5-7B-Instruct"
    
    try:
        # 1. åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_id,
            trust_remote_code=True
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        print("âœ… åˆ†è¯å™¨åŠ è½½å®Œæˆ")
        
        # 2. å†³å®šä½¿ç”¨æœ¬åœ°é€‚é…å™¨è¿˜æ˜¯HFé€‚é…å™¨
        if use_hf_adapter:
            adapter_id = "xueh88401/qwen2.5-7b-medical"
            print(f"ä½¿ç”¨HuggingFaceé€‚é…å™¨: {adapter_id}")
        else:
            adapter_id = adapter_path
            print(f"ä½¿ç”¨æœ¬åœ°é€‚é…å™¨: {adapter_path}")
        
        # 3. åŠ è½½åŸºç¡€æ¨¡å‹
        # æ³¨æ„ï¼šå¦‚æœä½ çš„æ˜¾å­˜å°äº16GBï¼Œå¯ä»¥å°è¯•æ·»åŠ  load_in_4bit=True æˆ– load_in_8bit=True
        model = AutoModelForCausalLM.from_pretrained(
            base_model_id,
            torch_dtype=torch.float16,
            device_map="auto",  # è‡ªåŠ¨åˆ†é…GPU/CPU
            trust_remote_code=True
        )
        print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # 4. åŠ è½½LoRAé€‚é…å™¨
        model = PeftModel.from_pretrained(model, adapter_id)
        model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        print("âœ… LoRAé€‚é…å™¨åŠ è½½å®Œæˆ")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("\nğŸ’¡ å¯èƒ½çš„åŸå› åŠè§£å†³æ–¹æ¡ˆ:")
        print("1. æ˜¾å­˜ä¸è¶³ -> æ·»åŠ å‚æ•°: load_in_4bit=True")
        print("2. ç½‘ç»œé—®é¢˜ -> ç¡®ä¿å¯ä»¥è®¿é—®HuggingFace")
        print("3. è·¯å¾„é”™è¯¯ -> æ£€æŸ¥adapter_pathå‚æ•°")
        sys.exit(1)

def generate_response(query, model, tokenizer, max_new_tokens=300):
    """
    ç”Ÿæˆå›å¤çš„æ ¸å¿ƒå‡½æ•°ã€‚
    å‚æ•°:
        query: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        model: åŠ è½½å¥½çš„æ¨¡å‹
        tokenizer: åŠ è½½å¥½çš„åˆ†è¯å™¨
        max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
    è¿”å›:
        æ¨¡å‹ç”Ÿæˆçš„å›å¤æ–‡æœ¬
    """
    # æ„å»ºç¬¦åˆQwen2.5å¯¹è¯æ ¼å¼çš„è¾“å…¥
    prompt = f"ç”¨æˆ·ï¼š{query}\nåŠ©æ‰‹ï¼š"
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆé…ç½®ï¼ˆä½ å¯ä»¥è°ƒæ•´è¿™äº›å‚æ•°ï¼‰
    generate_kwargs = {
        "max_new_tokens": max_new_tokens,
        "temperature": 0.7,      # æ§åˆ¶éšæœºæ€§ï¼šè¶Šä½è¶Šç¡®å®šï¼Œè¶Šé«˜è¶Šå¤šæ ·
        "top_p": 0.9,           # æ ¸é‡‡æ ·å‚æ•°
        "do_sample": True,      # å¯ç”¨é‡‡æ ·
        "repetition_penalty": 1.1,  # é‡å¤æƒ©ç½š
    }
    
    # å¼€å§‹ç”Ÿæˆ
    print("æ­£åœ¨ç”Ÿæˆå›å¤...")
    with torch.no_grad():
        outputs = model.generate(**inputs, **generate_kwargs)
    
    # è§£ç å¹¶æå–åŠ©æ‰‹çš„æ–°å›å¤éƒ¨åˆ†
    response = tokenizer.decode(
        outputs[0][inputs.input_ids.shape[1]:], 
        skip_special_tokens=True
    )
    
    return response.strip()

def main():
    """ä¸»å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿è¡Œæ¨ç†"""
    parser = argparse.ArgumentParser(description="Qwen2.5-7BåŒ»å­¦æ¨¡å‹æ¨ç†æ¼”ç¤º")
    parser.add_argument("--query", type=str, default="æ„Ÿå†’äº†æ€ä¹ˆåŠï¼Ÿ",
                       help="è¦å’¨è¯¢çš„åŒ»ç–—é—®é¢˜ï¼ˆé»˜è®¤ï¼š'æ„Ÿå†’äº†æ€ä¹ˆåŠï¼Ÿ'ï¼‰")
    parser.add_argument("--adapter_path", type=str, default="./adapter",
                       help="LoRAé€‚é…å™¨æœ¬åœ°è·¯å¾„ï¼ˆé»˜è®¤ï¼š'./adapter'ï¼‰")
    parser.add_argument("--hf", action="store_true",
                       help="ä½¿ç”¨HuggingFaceä¸Šçš„é€‚é…å™¨ï¼ˆè€Œä¸æ˜¯æœ¬åœ°æ–‡ä»¶ï¼‰")
    parser.add_argument("--max_tokens", type=int, default=300,
                       help="æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆé»˜è®¤ï¼š300ï¼‰")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æœ¬åœ°é€‚é…å™¨è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœä¸ä½¿ç”¨HFé€‚é…å™¨ï¼‰
    if not args.hf and not os.path.exists(args.adapter_path):
        print(f"âš ï¸  æœ¬åœ°é€‚é…å™¨è·¯å¾„ '{args.adapter_path}' ä¸å­˜åœ¨ã€‚")
        print("å°†è‡ªåŠ¨åˆ‡æ¢åˆ°HuggingFaceé€‚é…å™¨...")
        args.hf = True
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model_and_tokenizer(args.adapter_path, args.hf)
    
    print(f"\n{'='*50}")
    print(f"é—®é¢˜ï¼š{args.query}")
    print(f"{'='*50}")
    
    # ç”Ÿæˆå›å¤
    response = generate_response(args.query, model, tokenizer, args.max_tokens)
    
    print(f"\nğŸ¤– åŠ©æ‰‹å›å¤ï¼š\n{response}")
    print(f"\n{'='*50}")
    print(f"å›å¤é•¿åº¦ï¼š{len(response)} å­—ç¬¦")

if __name__ == "__main__":
    main()